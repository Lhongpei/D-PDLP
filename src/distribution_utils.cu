#include "cupdlpx_types.h"
#include "internal_types.h"
#include "distribution_utils.h"
#include "pdlp_core_op.h"
#include "utils.h"
#include <mpi.h>
#include <nccl.h>
#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define NCCL_CHECK(cmd) do {                         \
  ncclResult_t r = cmd;                              \
  if (r != ncclSuccess) {                            \
    printf("NCCL failure %s:%d '%s'\n",              \
        __FILE__, __LINE__, ncclGetErrorString(r));  \
    exit(EXIT_FAILURE);                              \
  }                                                  \
} while(0)

extern "C" {
ncclComm_t init_nccl(MPI_Comm mpi_comm) {
    ncclUniqueId id;
    ncclComm_t nccl_comm;
    int rank, nranks;

    MPI_Comm_rank(mpi_comm, &rank);
    MPI_Comm_size(mpi_comm, &nranks);

    if (rank == 0) {
        NCCL_CHECK(ncclGetUniqueId(&id));
    }

    MPI_Bcast(&id, sizeof(id), MPI_BYTE, 0, mpi_comm);
    NCCL_CHECK(ncclCommInitRank(&nccl_comm, nranks, id, rank));

    return nccl_comm;
}

grid_context_t initialize_parallel_context(int P_row, int P_col) {
    grid_context_t grid;
    int initialized;
    
    MPI_Initialized(&initialized);
    if (!initialized) {
        MPI_Init(NULL, NULL);
    }

    grid.comm_global = MPI_COMM_WORLD;
    MPI_Comm_rank(grid.comm_global, &grid.rank_global);
    
    grid.dims[0] = P_row;
    grid.dims[1] = P_col;

    int num_devices;
    CUDA_CHECK(cudaGetDeviceCount(&num_devices));
    int local_device_id = grid.rank_global % num_devices;
    CUDA_CHECK(cudaSetDevice(local_device_id));

    int my_row = grid.rank_global / P_col;
    int my_col = grid.rank_global % P_col;
    
    grid.coords[0] = my_row;
    grid.coords[1] = my_col;

    MPI_Comm_split(grid.comm_global, my_row, grid.rank_global, &grid.comm_row);
    MPI_Comm_split(grid.comm_global, my_col, grid.rank_global, &grid.comm_col);
    grid.nccl_row = init_nccl(grid.comm_row);
    grid.nccl_col = init_nccl(grid.comm_col);

    return grid;
}
}
int* get_balanced_cuts(const int* weights, int total_dim, int num_partitions) {
    int* cuts = (int*)malloc((num_partitions + 1) * sizeof(int));
    cuts[0] = 0; 
    cuts[num_partitions] = total_dim;

    if (num_partitions == 1) return cuts;

    long long total_weight = 0;
    for (int i = 0; i < total_dim; i++) total_weight += weights[i];

    double target_per_part = (double)total_weight / num_partitions;
    long long current_cumulative = 0;
    int partition_idx = 1;

    for (int i = 0; i < total_dim; i++) {
        current_cumulative += weights[i];

        if (current_cumulative >= partition_idx * target_per_part) {
            cuts[partition_idx] = i + 1;
            partition_idx++;
            if (partition_idx >= num_partitions) break;
        }
    }

    while (partition_idx < num_partitions) {
        cuts[partition_idx] = total_dim;
        partition_idx++;
    }

    return cuts;
}

void csr_extract_submatrix(
    int n_total, int m_total, 
    const int* A_row_ptr, const int* A_col_ind, const double* A_val,
    int row_start, int row_end,
    int col_start, int col_end,
    int** sub_row_ptr, int** sub_col_ind, double** sub_val, int* sub_nnz
) {
    if(n_total <= 0 || m_total <=0)
        return;
    int m_sub = row_end - row_start;
    
    int nnz_count = 0;
    for (int i = row_start; i < row_end; i++) {
        for (int jj = A_row_ptr[i]; jj < A_row_ptr[i+1]; jj++) {
            int col = A_col_ind[jj];
            if (col >= col_start && col < col_end) {
                nnz_count++;
            }
        }
    }
    *sub_nnz = nnz_count;

    *sub_row_ptr = (int*)malloc((m_sub + 1) * sizeof(int));
    *sub_col_ind = (int*)malloc(nnz_count * sizeof(int));
    *sub_val = (double*)malloc(nnz_count * sizeof(double));

    (*sub_row_ptr)[0] = 0;
    int current_nnz = 0;
    for (int i = row_start; i < row_end; i++) {
        for (int jj = A_row_ptr[i]; jj < A_row_ptr[i+1]; jj++) {
            int col = A_col_ind[jj];
            if (col >= col_start && col < col_end) {
                (*sub_col_ind)[current_nnz] = col - col_start; 
                (*sub_val)[current_nnz] = A_val[jj];
                current_nnz++;
            }
        }
        (*sub_row_ptr)[i - row_start + 1] = current_nnz;
    }
}

double* copy_slice(const double* src, int start, int count) {
    if (count <= 0) return NULL;
    double* dst = (double*)malloc(count * sizeof(double));
    memcpy(dst, src + start, count * sizeof(double));
    return dst;
}

lp_problem_t* partition_lp_problem(
    const lp_problem_t* global_lp, 
    const grid_context_t* grid, 
    partition_method_t method,
    int* out_n_start,  
    int* out_m_start 
) {
    lp_problem_t* loc = (lp_problem_t*)calloc(1, sizeof(lp_problem_t));

    int my_row_idx = grid->coords[0];
    int my_col_idx = grid->coords[1];
    int P_rows = grid->dims[0];
    int P_cols = grid->dims[1];

    int n_total = global_lp->num_variables;
    int m_total = global_lp->num_constraints;

    int n_start, n_end, m_start, m_end;

    if (method == NNZ_BALANCE_PARTITION) {
        int* col_weights = (int*)calloc(n_total, sizeof(int));
        for (int i = 0; i < global_lp->constraint_matrix_num_nonzeros; i++) {
            int c = global_lp->constraint_matrix_col_indices[i];
            if(c < n_total) col_weights[c]++;
        }
        int* col_cuts = get_balanced_cuts(col_weights, n_total, P_cols);
        n_start = col_cuts[my_col_idx];
        n_end   = col_cuts[my_col_idx + 1];
        free(col_weights); free(col_cuts);

        int* row_weights = (int*)malloc(m_total * sizeof(int));
        for (int i = 0; i < m_total; i++) {
            row_weights[i] = global_lp->constraint_matrix_row_pointers[i+1] 
                           - global_lp->constraint_matrix_row_pointers[i];
        }
        int* row_cuts = get_balanced_cuts(row_weights, m_total, P_rows);
        m_start = row_cuts[my_row_idx];
        m_end   = row_cuts[my_row_idx + 1];
        free(row_weights); free(row_cuts);

    } else {
        int n_chunk = n_total / P_cols;
        n_start = my_col_idx * n_chunk;
        n_end   = (my_col_idx == P_cols - 1) ? n_total : (my_col_idx + 1) * n_chunk;

        int m_chunk = m_total / P_rows;
        m_start = my_row_idx * m_chunk;
        m_end   = (my_row_idx == P_rows - 1) ? m_total : (my_row_idx + 1) * m_chunk;
    }

    if (out_n_start) *out_n_start = n_start;
    if (out_m_start) *out_m_start = m_start;

    loc->num_variables = n_end - n_start;
    loc->num_constraints = m_end - m_start;

    csr_extract_submatrix(
        n_total, m_total,
        global_lp->constraint_matrix_row_pointers,
        global_lp->constraint_matrix_col_indices,
        global_lp->constraint_matrix_values,
        m_start, m_end, n_start, n_end,
        &loc->constraint_matrix_row_pointers,
        &loc->constraint_matrix_col_indices,
        &loc->constraint_matrix_values,
        &loc->constraint_matrix_num_nonzeros
    );

    loc->objective_vector     = copy_slice(global_lp->objective_vector, n_start, loc->num_variables);
    loc->variable_lower_bound = copy_slice(global_lp->variable_lower_bound, n_start, loc->num_variables);
    loc->variable_upper_bound = copy_slice(global_lp->variable_upper_bound, n_start, loc->num_variables);
    
    loc->constraint_lower_bound = copy_slice(global_lp->constraint_lower_bound, m_start, loc->num_constraints);
    loc->constraint_upper_bound = copy_slice(global_lp->constraint_upper_bound, m_start, loc->num_constraints);

    if (global_lp->primal_start) {
        loc->primal_start = copy_slice(global_lp->primal_start, n_start, loc->num_variables);
    }
    if (global_lp->dual_start) {
        loc->dual_start = copy_slice(global_lp->dual_start, m_start, loc->num_constraints);
    }
    
    loc->objective_constant = global_lp->objective_constant; 
    return loc;
}

rescale_info_t* partition_rescale_info(
    rescale_info_t* global_info, 
    const grid_context_t* grid, 
    partition_method_t method,
    int* out_n_start, 
    int* out_m_start
) {
    rescale_info_t* loc_info = (rescale_info_t*)calloc(1, sizeof(rescale_info_t));
    
    int n_start, m_start;
    loc_info->scaled_problem = partition_lp_problem(
        global_info->scaled_problem, 
        grid, 
        method, 
        &n_start, 
        &m_start
    );

    lp_problem_t* loc_lp = loc_info->scaled_problem;

    loc_info->var_rescale = copy_slice(
        global_info->var_rescale, 
        n_start, 
        loc_lp->num_variables
    );

    loc_info->con_rescale = copy_slice(
        global_info->con_rescale, 
        m_start, 
        loc_lp->num_constraints
    );

    if (out_n_start) *out_n_start = n_start;
    if (out_m_start) *out_m_start = m_start;

    loc_info->con_bound_rescale  = global_info->con_bound_rescale;
    loc_info->obj_vec_rescale    = global_info->obj_vec_rescale;
    loc_info->rescaling_time_sec = global_info->rescaling_time_sec;

    return loc_info;
}

size_t get_lp_problem_size(const lp_problem_t *lp) {
    if (!lp) return 0;
    size_t size = 0;

    size += sizeof(int) * 3 + sizeof(double); 

    size += sizeof(double) * lp->num_variables * 3; 
    size += sizeof(double) * lp->num_constraints * 2; 

    size += sizeof(int) * (lp->num_constraints + 1);
    size += sizeof(int) * lp->constraint_matrix_num_nonzeros;
    size += sizeof(double) * lp->constraint_matrix_num_nonzeros;

    size += sizeof(int) * 2; 

    if (lp->primal_start) size += sizeof(double) * lp->num_variables;
    if (lp->dual_start)   size += sizeof(double) * lp->num_constraints;

    return size;
}

void serialize_lp_problem_to_ptr(const lp_problem_t *lp, char **ptr_ref) {
    char *ptr = *ptr_ref;

    #define S_COPY(val, type) { *((type*)ptr) = val; ptr += sizeof(type); }
    #define S_ARR(arr, count, type) { memcpy(ptr, arr, sizeof(type)*(count)); ptr += sizeof(type)*(count); }

    S_COPY(lp->num_variables, int);
    S_COPY(lp->num_constraints, int);
    S_COPY(lp->constraint_matrix_num_nonzeros, int);
    S_COPY(lp->objective_constant, double);

    S_ARR(lp->objective_vector, lp->num_variables, double);
    S_ARR(lp->variable_lower_bound, lp->num_variables, double);
    S_ARR(lp->variable_upper_bound, lp->num_variables, double);
    S_ARR(lp->constraint_lower_bound, lp->num_constraints, double);
    S_ARR(lp->constraint_upper_bound, lp->num_constraints, double);

    S_ARR(lp->constraint_matrix_row_pointers, lp->num_constraints + 1, int);
    S_ARR(lp->constraint_matrix_col_indices, lp->constraint_matrix_num_nonzeros, int);
    S_ARR(lp->constraint_matrix_values, lp->constraint_matrix_num_nonzeros, double);

    int has_primal = (lp->primal_start != NULL);
    int has_dual = (lp->dual_start != NULL);
    S_COPY(has_primal, int);
    S_COPY(has_dual, int);

    if (has_primal) S_ARR(lp->primal_start, lp->num_variables, double);
    if (has_dual)   S_ARR(lp->dual_start, lp->num_constraints, double);

    *ptr_ref = ptr; 
}

lp_problem_t* deserialize_lp_problem_from_ptr(const char **ptr_ref) {
    const char *ptr = *ptr_ref;
    lp_problem_t *lp = (lp_problem_t*)calloc(1, sizeof(lp_problem_t));

    #define D_VAL(var, type) { var = *((type*)ptr); ptr += sizeof(type); }
    #define D_ARR(dest, count, type) { \
        dest = (type*)malloc(sizeof(type)*(count)); \
        memcpy(dest, ptr, sizeof(type)*(count)); \
        ptr += sizeof(type)*(count); \
    }

    D_VAL(lp->num_variables, int);
    D_VAL(lp->num_constraints, int);
    D_VAL(lp->constraint_matrix_num_nonzeros, int);
    D_VAL(lp->objective_constant, double);

    D_ARR(lp->objective_vector, lp->num_variables, double);
    D_ARR(lp->variable_lower_bound, lp->num_variables, double);
    D_ARR(lp->variable_upper_bound, lp->num_variables, double);
    D_ARR(lp->constraint_lower_bound, lp->num_constraints, double);
    D_ARR(lp->constraint_upper_bound, lp->num_constraints, double);

    D_ARR(lp->constraint_matrix_row_pointers, lp->num_constraints + 1, int);
    D_ARR(lp->constraint_matrix_col_indices, lp->constraint_matrix_num_nonzeros, int);
    D_ARR(lp->constraint_matrix_values, lp->constraint_matrix_num_nonzeros, double);

    int has_primal, has_dual;
    D_VAL(has_primal, int);
    D_VAL(has_dual, int);

    if (has_primal) D_ARR(lp->primal_start, lp->num_variables, double);
    if (has_dual)   D_ARR(lp->dual_start, lp->num_constraints, double);

    *ptr_ref = ptr;
    return lp;
}


size_t get_rescale_info_size(const rescale_info_t *info) {
    if (!info) return 0;
    size_t size = 0;
    size += sizeof(double) * 3; 
    int n = info->scaled_problem->num_variables;
    int m = info->scaled_problem->num_constraints;
    size += sizeof(double) * (n + m);

    size += get_lp_problem_size(info->scaled_problem);
    
    return size;
}

void serialize_rescale_info(const rescale_info_t *info, char *buffer) {
    char *ptr = buffer;
    
    S_COPY(info->con_bound_rescale, double);
    S_COPY(info->obj_vec_rescale, double);
    S_COPY(info->rescaling_time_sec, double);

    serialize_lp_problem_to_ptr(info->scaled_problem, &ptr);

    int n = info->scaled_problem->num_variables;
    int m = info->scaled_problem->num_constraints;
    S_ARR(info->var_rescale, n, double);
    S_ARR(info->con_rescale, m, double);
}

rescale_info_t* deserialize_rescale_info(const char *buffer) {
    const char *ptr = buffer;
    rescale_info_t *info = (rescale_info_t*)calloc(1, sizeof(rescale_info_t));

    D_VAL(info->con_bound_rescale, double);
    D_VAL(info->obj_vec_rescale, double);
    D_VAL(info->rescaling_time_sec, double);

    info->scaled_problem = deserialize_lp_problem_from_ptr(&ptr);

    int n = info->scaled_problem->num_variables;
    int m = info->scaled_problem->num_constraints;
    D_ARR(info->var_rescale, n, double);
    D_ARR(info->con_rescale, m, double);

    return info;
}

#define CHUNK_SIZE (1024 * 1024 * 1024) 

void big_bcast_bytes(void **buffer_ptr, size_t *size_ptr, int root, MPI_Comm comm) {
    int rank;
    MPI_Comm_rank(comm, &rank);
    int is_root = (rank == root);

    unsigned long long total_len = is_root ? *size_ptr : 0;
    MPI_Bcast(&total_len, 1, MPI_UNSIGNED_LONG_LONG, root, comm);

    if (!is_root) {
        *size_ptr = (size_t)total_len;
        *buffer_ptr = malloc(total_len);
    }

    char *buf = (char *)(*buffer_ptr);
    size_t offset = 0;
    
    while (offset < total_len) {
        size_t remaining = total_len - offset;
        int current_chunk = (remaining > CHUNK_SIZE) ? CHUNK_SIZE : (int)remaining;

        MPI_Bcast(buf + offset, current_chunk, MPI_BYTE, root, comm);
        
        offset += current_chunk;
    }
}

double compute_global_norm(cublasHandle_t blas_handle, int m_local, double *d_vec, MPI_Comm comm) {
    double local_norm_sq = 0.0;
    double global_norm_sq = 0.0;
    
    CUBLAS_CHECK(cublasDdot(blas_handle, m_local, d_vec, 1, d_vec, 1, &local_norm_sq));
    
    MPI_Allreduce(&local_norm_sq, &global_norm_sq, 1, MPI_DOUBLE, MPI_SUM, comm);
    
    return sqrt(global_norm_sq);
}

double compute_global_dot(cublasHandle_t blas_handle, int m_local, double *d_vec1, double *d_vec2, MPI_Comm comm) {
    double local_dot = 0.0;
    double global_dot = 0.0;

    CUBLAS_CHECK(cublasDdot(blas_handle, m_local, d_vec1, 1, d_vec2, 1, &local_dot));
    MPI_Allreduce(&local_dot, &global_dot, 1, MPI_DOUBLE, MPI_SUM, comm);
    
    return global_dot;
}

double estimate_maximum_singular_value_distributed(
    cusparseHandle_t sparse_handle,
    cublasHandle_t blas_handle,
    const grid_context_t *grid_context,
    const cu_sparse_matrix_csr_t *A,
    const cu_sparse_matrix_csr_t *AT,
    int max_iterations, double tolerance)
{
    int m = A->num_rows; 
    int n = A->num_cols; 
    double *eigenvector_d, *next_eigenvector_d, *dual_product_d;

    CUDA_CHECK(cudaMalloc((void**)&eigenvector_d, m * sizeof(double)));
    CUDA_CHECK(cudaMalloc((void**)&next_eigenvector_d, m * sizeof(double)));
    CUDA_CHECK(cudaMalloc((void**)&dual_product_d, n * sizeof(double)));

    double *eigenvector_h = (double *)safe_malloc(m * sizeof(double));

    unsigned int seed = 1234 + grid_context->coords[0]; 
    for (int i = 0; i < m; ++i) {
        eigenvector_h[i] = (double)rand_r(&seed) / RAND_MAX;
    }
    CUDA_CHECK(cudaMemcpy(eigenvector_d, eigenvector_h, m * sizeof(double), cudaMemcpyHostToDevice));
    free(eigenvector_h);

    double sigma_max_sq = 1.0;
    const double one = 1.0;
    const double zero = 0.0;

    cusparseSpMatDescr_t matA, matAT;
    CUSPARSE_CHECK(cusparseCreateCsr(&matA, A->num_rows, A->num_cols, A->num_nonzeros, 
                                     A->row_ptr, A->col_ind, A->val, 
                                     CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I, 
                                     CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));
    CUSPARSE_CHECK(cusparseCreateCsr(&matAT, AT->num_rows, AT->num_cols, AT->num_nonzeros, 
                                     AT->row_ptr, AT->col_ind, AT->val, 
                                     CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I, 
                                     CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));

    cusparseDnVecDescr_t vecEigen, vecNextEigen, vecDual;
    CUSPARSE_CHECK(cusparseCreateDnVec(&vecEigen, m, eigenvector_d, CUDA_R_64F));
    CUSPARSE_CHECK(cusparseCreateDnVec(&vecNextEigen, m, next_eigenvector_d, CUDA_R_64F));
    CUSPARSE_CHECK(cusparseCreateDnVec(&vecDual, n, dual_product_d, CUDA_R_64F));

    void *dBufferAT = NULL, *dBufferA = NULL;
    size_t bufferSizeAT = 0, bufferSizeA = 0;
    CUSPARSE_CHECK(cusparseSpMV_bufferSize(sparse_handle, CUSPARSE_OPERATION_NON_TRANSPOSE, 
                                           &one, matAT, vecEigen, &zero, vecDual, 
                                           CUDA_R_64F, CUSPARSE_SPMV_CSR_ALG2, &bufferSizeAT));
    CUSPARSE_CHECK(cusparseSpMV_bufferSize(sparse_handle, CUSPARSE_OPERATION_NON_TRANSPOSE, 
                                           &one, matA, vecDual, &zero, vecNextEigen, 
                                           CUDA_R_64F, CUSPARSE_SPMV_CSR_ALG2, &bufferSizeA));
    CUDA_CHECK(cudaMalloc((void**)&dBufferAT, bufferSizeAT));
    CUDA_CHECK(cudaMalloc((void**)&dBufferA, bufferSizeA));

    double local_norm;
    CUBLAS_CHECK(cublasDnrm2_v2_64(blas_handle, m, eigenvector_d, 1, &local_norm));
    double local_norm_sq = local_norm * local_norm;
    double global_norm_sq = 0.0;

    MPI_Allreduce(&local_norm_sq, &global_norm_sq, 1, MPI_DOUBLE, MPI_SUM, grid_context->comm_col);
    
    double inv_norm = 1.0 / sqrt(global_norm_sq);
    CUBLAS_CHECK(cublasDscal(blas_handle, m, &inv_norm, eigenvector_d, 1));

   for (int i = 0; i < max_iterations; ++i)
    {
        CUSPARSE_CHECK(cusparseSpMV(sparse_handle, CUSPARSE_OPERATION_NON_TRANSPOSE,
                                    &one, matAT, vecEigen, &zero, vecDual,
                                    CUDA_R_64F, CUSPARSE_SPMV_CSR_ALG2, dBufferAT));
        
        NCCL_CHECK(ncclAllReduce((const void *)dual_product_d, (void *)dual_product_d, n, 
                                 ncclDouble, ncclSum, grid_context->nccl_col, 0));

        CUSPARSE_CHECK(cusparseSpMV(sparse_handle, CUSPARSE_OPERATION_NON_TRANSPOSE,
                                    &one, matA, vecDual, &zero, vecNextEigen,
                                    CUDA_R_64F, CUSPARSE_SPMV_CSR_ALG2, dBufferA));

        NCCL_CHECK(ncclAllReduce((const void *)next_eigenvector_d, (void *)next_eigenvector_d, m, 
                                 ncclDouble, ncclSum, grid_context->nccl_row, 0));

        double local_dot;
        CUBLAS_CHECK(cublasDdot(blas_handle, m, next_eigenvector_d, 1, eigenvector_d, 1, &local_dot));
        
        MPI_Allreduce(&local_dot, &sigma_max_sq, 1, MPI_DOUBLE, MPI_SUM, grid_context->comm_col);

        double neg_sigma_sq = -sigma_max_sq;
        CUBLAS_CHECK(cublasDaxpy(blas_handle, m, &neg_sigma_sq, eigenvector_d, 1, next_eigenvector_d, 1));

        double local_res_norm;
        CUBLAS_CHECK(cublasDnrm2_v2_64(blas_handle, m, next_eigenvector_d, 1, &local_res_norm));
        
        double local_res_sq = local_res_norm * local_res_norm;
        double global_res_sq = 0.0;
        MPI_Allreduce(&local_res_sq, &global_res_sq, 1, MPI_DOUBLE, MPI_SUM, grid_context->comm_col);
        
        double residual_norm = sqrt(global_res_sq);

        if (residual_norm < tolerance) {
            break; 
        }

        CUBLAS_CHECK(cublasDaxpy(blas_handle, m, &sigma_max_sq, eigenvector_d, 1, next_eigenvector_d, 1));

        double local_norm;
        CUBLAS_CHECK(cublasDnrm2_v2_64(blas_handle, m, next_eigenvector_d, 1, &local_norm));
        double local_norm_sq = local_norm * local_norm;
        double global_norm_sq = 0.0;
        MPI_Allreduce(&local_norm_sq, &global_norm_sq, 1, MPI_DOUBLE, MPI_SUM, grid_context->comm_col);
        
        double inv_norm = 1.0 / sqrt(global_norm_sq);
        CUBLAS_CHECK(cublasDscal(blas_handle, m, &inv_norm, next_eigenvector_d, 1));

        double *tmp = eigenvector_d;
        eigenvector_d = next_eigenvector_d;
        next_eigenvector_d = tmp;

        CUSPARSE_CHECK(cusparseDnVecSetValues(vecEigen, eigenvector_d));
        CUSPARSE_CHECK(cusparseDnVecSetValues(vecNextEigen, next_eigenvector_d));
    }

    CUDA_CHECK(cudaFree(dBufferAT));
    CUDA_CHECK(cudaFree(dBufferA));
    CUSPARSE_CHECK(cusparseDestroySpMat(matA));
    CUSPARSE_CHECK(cusparseDestroySpMat(matAT));
    CUSPARSE_CHECK(cusparseDestroyDnVec(vecEigen));
    CUSPARSE_CHECK(cusparseDestroyDnVec(vecNextEigen));
    CUSPARSE_CHECK(cusparseDestroyDnVec(vecDual));
    CUDA_CHECK(cudaFree(eigenvector_d));
    
    return sqrt(sigma_max_sq);
}

void initialize_step_size_and_primal_weight_distributed(
    pdhg_solver_state_t *state,
    const pdhg_parameters_t *params)
{
    long long local_nnz = state->constraint_matrix->num_nonzeros;
    long long global_nnz = 0;
    
    MPI_Allreduce(&local_nnz, &global_nnz, 1, MPI_LONG_LONG, MPI_SUM, state->grid_context->comm_global);

    if (global_nnz == 0)
    {
        state->step_size = 1.0;
    }
    else
    {
        double max_sv = estimate_maximum_singular_value_distributed(
            state->sparse_handle, state->blas_handle, state->grid_context, state->constraint_matrix,
            state->constraint_matrix_t, params->sv_max_iter, params->sv_tol);
        MPI_Barrier(state->grid_context->comm_global);
        
        if (max_sv < 1e-9) max_sv = 1e-9;
        state->step_size = 0.998 / max_sv;
    }

    if (params->bound_objective_rescaling)
    {
        state->primal_weight = 1.0;
    }
    else
    {
        state->primal_weight = (state->objective_vector_norm + 1.0) /
                               (state->constraint_bound_norm + 1.0);
    }
    state->best_primal_weight = state->primal_weight;
}

void gather_distributed_vector(
    double *d_local_vec,
    int local_len,
    MPI_Comm comm_check, 
    MPI_Comm comm_gather,
    double **result_ptr
) {
    int rank_check;
    MPI_Comm_rank(comm_check, &rank_check);

    if (rank_check == 0) {
        double *h_local = (double*)malloc(local_len * sizeof(double));
        CUDA_CHECK(cudaMemcpy(h_local, d_local_vec, local_len * sizeof(double), cudaMemcpyDeviceToHost));

        int size_gather, rank_gather;
        MPI_Comm_size(comm_gather, &size_gather);
        MPI_Comm_rank(comm_gather, &rank_gather);

        int *counts = NULL;
        int *displs = NULL;
        double *h_global = NULL;

        if (rank_gather == 0) {
            counts = (int*)malloc(size_gather * sizeof(int));
            displs = (int*)malloc(size_gather * sizeof(int));
        }

        MPI_Gather(&local_len, 1, MPI_INT, counts, 1, MPI_INT, 0, comm_gather);

        if (rank_gather == 0) {
            int total_len = 0;
            for(int i=0; i<size_gather; ++i) {
                displs[i] = total_len;
                total_len += counts[i];
            }
            h_global = (double*)malloc(total_len * sizeof(double));
        }
        MPI_Gatherv(h_local, local_len, MPI_DOUBLE, 
                    h_global, counts, displs, MPI_DOUBLE, 
                    0, comm_gather);

        free(h_local);
        if (counts) free(counts);
        if (displs) free(displs);

        if (rank_gather == 0 && result_ptr != NULL) {
            *result_ptr = h_global; 
        } else if (h_global) {
             free(h_global);
        }
    }
}

void print_distributed_params(const pdhg_parameters_t *params)
{
    if (!params->verbose) return;
    printf("------------------------------ Distributed Configuration ------------------------------\n");

    if (params->grid_size.decided) {
        printf("   Grid Size         : %d x %d (Rows x Cols)\n", 
               params->grid_size.row_dims, params->grid_size.col_dims);
    } else {
        printf("   Grid Size         : Auto-detect (implementation dependent)\n");
    }

    printf("  Partition Method   : ");
    switch (params->partition_method)
    {
    case UNIFORM_PARTITION:
        printf("Uniform\n");
        break;
    case NNZ_BALANCE_PARTITION:
        printf("NNZ Balance\n");
        break;
    default:
        printf("Unknown (%d)\n", params->partition_method);
        break;
    }

    printf("  Permute Method     : ");
    switch (params->permute_method)
    {
    case NO_PERMUTATION:
        printf("None (Original ordering)\n");
        break;
    case FULL_RANDOM_PERMUTATION:
        printf("Full Random (Full Random shuffle)\n");
        break;
    case BLOCK_RANDOM_PERMUTATION:
        printf("Block Random (Block-wise Random shuffle)\n");
        break;
    default:
        printf("Unknown (%d)\n", params->permute_method);
        break;
    }
    
    printf("---------------------------------------------------------------------------------------\n\n");
}
